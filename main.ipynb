{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb195bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"PINECONE_API_KEY is not found in the .env file\")\n",
    "\n",
    "try:\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to initialize the pinecone client{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e17d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the embeddings\n",
    "class NormalizedEmbeddings(HuggingFaceEmbeddings):\n",
    "    def embed_query(self, text):\n",
    "        vec = super().embed_query(text)\n",
    "        return (vec / np.linalg.norm(vec)).tolist()\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        vecs = super().embed_documents(texts)\n",
    "        return [(v / np.linalg.norm(v)).tolist() for v in vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff146d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists\n",
      "Loaded 1 document(s)\n",
      "Total characters in document is: 3765\n",
      "split into 9 chunks\n",
      "Chunk 1 length: 443 characters\n",
      "Chunk 2 length: 422 characters\n",
      "Chunk 3 length: 451 characters\n",
      "Chunk 4 length: 487 characters\n",
      "Chunk 5 length: 493 characters\n",
      "Chunk 6 length: 494 characters\n",
      "Chunk 7 length: 440 characters\n",
      "Chunk 8 length: 472 characters\n",
      "Chunk 9 length: 72 characters\n"
     ]
    }
   ],
   "source": [
    "index_name = \"first-pc-rag\"\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "    print(\"Index already exists\")\n",
    "else:\n",
    "    print(\"Index does not exists\")\n",
    "\n",
    "# connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# delete vectors in index (this is needed when I want to change the number of chunks)\n",
    "# index.delete(delete_all=True)\n",
    "\n",
    "# load and split document\n",
    "loader = TextLoader(\"policies.md\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Total characters in document is: {len(documents[0].page_content)}\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"split into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} length: {len(chunk.page_content)} characters\")\n",
    "\n",
    "embeddings_model = NormalizedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "vectors = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    vector = embeddings_model.embed_query(chunk.page_content)\n",
    "    vectors.append({\n",
    "        \"id\": f\"chunk-{i}\",\n",
    "        \"values\": vector,\n",
    "        \"metadata\": {\n",
    "            \"text\": chunk.page_content,\n",
    "            \"source\": chunk.metadata.get(\"source\", \"policies.md\")\n",
    "        }\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af1992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted batch 1/1\n",
      "successfully upserted 9 vectors\n"
     ]
    }
   ],
   "source": [
    "# upserting into pinecone\n",
    "batch_size=100\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    batch = vectors[i:1 + batch_size]\n",
    "    index.upsert(vectors=batch)\n",
    "    print(f\"Upserted batch {i//batch_size + 1}/{(len(vectors)-1)//batch_size + 1}\")\n",
    "print(f\"successfully upserted {len(vectors)} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4908e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what does the company say about mobile phones\"\n",
    "query_vector = embeddings_model.embed_query(query)\n",
    "\n",
    "results = index.query(\n",
    "    vector=query_vector,\n",
    "    top_k=3,\n",
    "    include_metadata=True\n",
    ")\n",
    "# if results['matches']:\n",
    "#     for match in results['matches']:\n",
    "#         print(f\"score: {match['score']:.4f}\")\n",
    "#         print(f\"Text: {match['metadata']['text'][:200]}...\\n\")\n",
    "# else: \n",
    "#     print(\"No matches found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3669229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import  PromptTemplate     \n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_pinecone import Pinecone                   # WORKS\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index=index,\n",
    "    embedding=embeddings_model,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "model_id = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.3,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d2bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: You are a helpful assistant for DelishDine Restaurants Ltd.\n",
      "Answer the question using ONLY the context below. If unsure, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      "# DelishDine Restaurants Ltd. â€“ Company Policy Manual\n",
      "\n",
      "## Company Name\n",
      "The name of our company is **DelishDine Restaurants Ltd.**\n",
      "\n",
      "## 1. Introduction\n",
      "Welcome to DelishDine Restaurants Ltd.\n",
      "\n",
      "This policy manual outlines the standards, rules, and expectations that guide how we work together to deliver exceptional dining experiences to our customers. All employees are expected to read, understand, and comply with the policies described herein.\n",
      "\n",
      "**Sharing company secrets with outside parties is strictly prohibited.**\n",
      "\n",
      "### Reporting Hazards\n",
      "Report any equipment malfunctions or food safety hazards immediately to the supervisor.\n",
      "\n",
      "## 7. Customer Service Policy\n",
      "\n",
      "### Greeting\n",
      "Greet every customer with a smile and a warm welcome.\n",
      "\n",
      "### Handling Concerns\n",
      "Address all customer concerns promptly and courteously.\n",
      "\n",
      "### Escalation\n",
      "**Never argue with a customer**; escalate serious complaints to the restaurant manager.\n",
      "\n",
      "### Confidentiality\n",
      "Maintain confidentiality regarding customer information.\n",
      "\n",
      "## 8. Health and Safety\n",
      "\n",
      "Question: what is the company name\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# prompt template\n",
    "# === 3. Prompt Template ===\n",
    "template = \"\"\"You are a helpful assistant for DelishDine Restaurants Ltd.\n",
    "Answer the question using ONLY the context below. If unsure, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"what is the company name\"\n",
    "answer = rag_chain.invoke(query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
